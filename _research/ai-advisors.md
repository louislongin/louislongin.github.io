---
title: "AI Advisors"
layout: single-portfolio
excerpt: "<img src='/images/ai-advisor.png' alt='ai-advisor-project'>"
collection: research
order_number: 20
header: 
  og_image: "research/ai-advisor.png"
---

The rapid advancement of AI technology has raised critical questions about how humans perceive and interact with AI systems, particularly in advisory roles. While previous research has focused on fully autonomous AI agents, less is known about how humans attribute responsibility and agency to AI systems that serve purely instrumental or advisory functions. This research project investigates whether even basic AI assistants are perceived as sharing responsibility with their human users, despite being designed as mere tools.

Through carefully designed experimental studies comparing AI-powered and non-AI-powered advisory systems, the research demonstrates that the mere presence of AI fundamentally changes how responsibility is attributed - even when the AI system provides identical information as a mechanical tool. The findings reveal a striking paradox: while people consciously view AI assistants as tools, they unconsciously treat them as agents capable of sharing responsibility. This is evidenced by reduced responsibility attributed to human users when working with AI advisors compared to non-AI tools, and asymmetric praise/blame patterns typically associated with human agents.

This work makes important contributions to understanding human-AI interaction by showing that the involvement of AI, even in basic advisory roles, introduces implicit attributions of agency and responsibility sharing that go beyond the systemâ€™s designed instrumental function. These insights are particularly relevant as AI assistants become increasingly prevalent across domains from driving to healthcare, highlighting the need to carefully consider how humans naturally perceive and respond to AI advisory systems when designing human-AI interactions.

The research helps fill a critical gap in our understanding of human-AI relationships by demonstrating that responsibility attribution to AI systems does not require explicit agency or autonomy, but emerges from the mere involvement of artificial intelligence in advisory roles. This has important implications for the design and deployment of AI assistants across various domains.

## Publications

Longin, L. (2023). The Social Role of AI Advisors. 

[Article](https://edoc.ub.uni-muenchen.de/32546/){: .btn--research}

Longin, L., Bahrami, B., & Deroy, O. (2023). Intelligence brings responsibility-Even smart AI assistants are held responsible. Iscience, 26(8).

[Article](https://www.cell.com/iscience/fulltext/S2589-0042(23)01571-7){: .btn--research}
[Interview](https://mi3.info/blog-post/the-blurred-lines-of-responsibility-how-people-perceive-ai-assistants/){: .btn--research}

Longin, L., & Deroy, O. (2022). Augmenting perception: How artificial intelligence transforms sensory substitution. Consciousness and Cognition, 99, 103280. 

[Article](https://www.sciencedirect.com/science/article/pii/S1053810022000125){: .btn--research}

Longin, L. (2020). Towards a Middle-Ground Theory of Agency for Artificial Intelligence. In Culturally Sustainable Social Robotics (pp. 17-26). IOS Press. 

[Article](https://ebooks.iospress.nl/doi/10.3233/FAIA200897){: .btn--research}

